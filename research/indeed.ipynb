{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import smtplib\n",
    "from email import encoders\n",
    "from email.mime.base import MIMEBase\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium_stealth import stealth\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "\n",
    "global total_jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_webdriver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--start-maximized\")  # Open browser in maximized mode\n",
    "    options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    options.add_experimental_option('useAutomationExtension', False)\n",
    "    driver = webdriver.Chrome(executable_path=\"D:\\Python\\scraping\\IndeedJobScrap\\chromedriver.exe\", options=options)\n",
    "\n",
    "    stealth(driver,\n",
    "            languages=[\"en-US\", \"en\"],\n",
    "            vendor=\"Google Inc.\",\n",
    "            platform=\"Win32\",\n",
    "            webgl_vendor=\"Intel Inc.\",\n",
    "            renderer=\"Intel Iris OpenGL Engine\",\n",
    "            fix_hairline=True,\n",
    "            )\n",
    "\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sharif\\AppData\\Local\\Temp\\ipykernel_22564\\963482279.py:7: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path=\"D:\\Python\\scraping\\IndeedJobScrap\\chromedriver.exe\", options=options)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.chrome.webdriver.WebDriver (session=\"211011175c7b38de90abbb02e897f1d0\")>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configure_webdriver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_jobs(driver, country, job_position, job_location, date_posted):\n",
    "    full_url = f'{country}/jobs?q={\"+\".join(job_position.split())}&l={job_location}&fromage={date_posted}'\n",
    "    print(full_url)\n",
    "    driver.get(full_url)\n",
    "    time.sleep(20)\n",
    "    global total_jobs\n",
    "    try:\n",
    "        job_count_element = driver.find_element(By.XPATH,\n",
    "                                                '//div[starts-with(@class, \"jobsearch-JobCountAndSortPane-jobCount\")]')\n",
    "        total_jobs = job_count_element.find_element(By.XPATH, './span').text\n",
    "        print(f\"{total_jobs} found\")\n",
    "    except NoSuchElementException:\n",
    "        print(\"No job count found\")\n",
    "        total_jobs = \"Unknown\"\n",
    "\n",
    "    driver.save_screenshot('screenshot.png')\n",
    "    return full_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_job_data(driver, country):\n",
    "    df = pd.DataFrame({'Link': [''], 'Job Title': [''], 'Company': [''],\n",
    "                       'Employer Active': [''], 'Location': ['']})\n",
    "    job_count = 0\n",
    "    # count = 0\n",
    "    while True:\n",
    "        # count += 1\n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "        boxes = soup.find_all('div', class_='job_seen_beacon')\n",
    "\n",
    "        for i in boxes:\n",
    "            try:\n",
    "                link = i.find('a', {'data-jk': True}).get('href')\n",
    "                link_full = country + link\n",
    "            except (AttributeError, TypeError):\n",
    "                try:\n",
    "                    link = i.find('a', class_=lambda x: x and 'JobTitle' in x).get('href')\n",
    "                    link_full = country + link\n",
    "                except (AttributeError, TypeError):\n",
    "                    link_full = None\n",
    "\n",
    "            try:\n",
    "                job_title = i.find('a', class_=lambda x: x and 'JobTitle' in x).text.strip()\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    job_title = i.find('span', id=lambda x: x and 'jobTitle-' in str(x)).text.strip()\n",
    "                except AttributeError:\n",
    "                    job_title = None\n",
    "\n",
    "            try:\n",
    "                company = i.find('span', {'data-testid': 'company-name'}).text.strip()\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    company = i.find('span', class_=lambda x: x and 'company' in str(x).lower()).text.strip()\n",
    "                except AttributeError:\n",
    "                    company = None\n",
    "\n",
    "            try:\n",
    "                employer_active = i.find('span', class_='date').text.strip()\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    employer_active = i.find('span', {'data-testid': 'myJobsStateDate'}).text.strip()\n",
    "                except AttributeError:\n",
    "                    employer_active = None\n",
    "\n",
    "            try:\n",
    "                location_element = i.find('div', {'data-testid': 'text-location'})\n",
    "                if location_element:\n",
    "                    try:\n",
    "                        location = location_element.find('span').text.strip()\n",
    "                    except AttributeError:\n",
    "                        location = location_element.text.strip()\n",
    "                else:\n",
    "                    raise AttributeError\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    location_element = i.find('div', class_=lambda x: x and 'location' in str(x).lower())\n",
    "                    if location_element:\n",
    "                        try:\n",
    "                            location = location_element.find('span').text.strip()\n",
    "                        except AttributeError:\n",
    "                            location = location_element.text.strip()\n",
    "                    else:\n",
    "                        location = ''\n",
    "                except AttributeError:\n",
    "                    location = ''\n",
    "\n",
    "            new_data = pd.DataFrame({'Link': [link_full], 'Job Title': [job_title],\n",
    "                                     'Company': [company],\n",
    "                                     'Employer Active': [employer_active],\n",
    "                                     'Location': [location]})\n",
    "\n",
    "            df = pd.concat([df, new_data], ignore_index=True)\n",
    "            job_count += 1\n",
    "\n",
    "        print(f\"Scraped {job_count} of {total_jobs}\")\n",
    "\n",
    "        try:\n",
    "            next_page = soup.find('a', {'aria-label': 'Next Page'}).get('href')\n",
    "\n",
    "            next_page = country + next_page\n",
    "            driver.get(next_page)\n",
    "\n",
    "        except:\n",
    "            print('got issue in next page')\n",
    "            break\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_data(df):\n",
    "    def posted(x):\n",
    "        try:\n",
    "            x = x.replace('EmployerActive', '').strip()\n",
    "            return x\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    df['Employer Active'] = df['Employer Active'].apply(posted)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv(df, job_position, job_location):\n",
    "    def get_user_desktop_path():\n",
    "        home_dir = os.path.expanduser(\"~\")\n",
    "        desktop_path = os.path.join(home_dir, \"Desktop\")\n",
    "        return desktop_path\n",
    "\n",
    "    file_path = os.path.join(get_user_desktop_path(), '{}_{}'.format(job_position, job_location))\n",
    "    df = pd.DataFrame(df)\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\"\"\"\n",
    "List of countries url.\n",
    "\"\"\"\n",
    "nigeria = 'https://ng.indeed.com'\n",
    "united_kingdom = 'https://uk.indeed.com'\n",
    "united_states = 'https://www.indeed.com'\n",
    "canada = 'https://ca.indeed.com'\n",
    "germany = 'https://de.indeed.com'\n",
    "australia = 'https://au.indeed.com'\n",
    "south_africa = 'https://za.indeed.com'\n",
    "sweden = 'https://se.indeed.com'\n",
    "singapore = 'https://www.indeed.com.sg'\n",
    "switzerland = 'https://www.indeed.ch'\n",
    "united_arab_emirates = 'https://www.indeed.ae'\n",
    "new_zealand = 'https://nz.indeed.com'\n",
    "india = 'https://www.indeed.co.in'\n",
    "france = 'https://www.indeed.fr'\n",
    "italy = 'https://it.indeed.com'\n",
    "spain = 'https://www.indeed.es'\n",
    "japan = 'https://jp.indeed.com'\n",
    "south_korea = 'https://kr.indeed.com'\n",
    "brazil = 'https://www.indeed.com.br'\n",
    "mexico = 'https://www.indeed.com.mx'\n",
    "china = 'https://cn.indeed.com'\n",
    "saudi_arabia = 'https://sa.indeed.com'\n",
    "egypt = 'https://eg.indeed.com'\n",
    "thailand = 'https://th.indeed.com'\n",
    "vietnam = 'https://vn.indeed.com'\n",
    "argentina = 'https://ar.indeed.com'\n",
    "ireland = 'https://ie.indeed.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    driver = configure_webdriver()\n",
    "    country = india\n",
    "    job_position = 'java'\n",
    "    job_location = 'remote'\n",
    "    date_posted = 20\n",
    "\n",
    "    try:\n",
    "        full_url = search_jobs(driver, country, job_position, job_location, date_posted)\n",
    "        df = scrape_job_data(driver, country) \n",
    "        save_csv(df,job_position, job_location)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        driver.quit()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sharif\\AppData\\Local\\Temp\\ipykernel_22564\\963482279.py:7: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path=\"D:\\Python\\scraping\\IndeedJobScrap\\chromedriver.exe\", options=options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.indeed.co.in/jobs?q=java&l=remote&fromage=20\n",
      "100+ jobs found\n",
      "Scraped 15 of 100+ jobs\n",
      "Scraped 30 of 100+ jobs\n",
      "Scraped 45 of 100+ jobs\n",
      "Scraped 60 of 100+ jobs\n",
      "Scraped 75 of 100+ jobs\n",
      "Scraped 90 of 100+ jobs\n",
      "Scraped 105 of 100+ jobs\n",
      "Scraped 120 of 100+ jobs\n",
      "Scraped 135 of 100+ jobs\n",
      "Scraped 135 of 100+ jobs\n",
      "got issue in next page\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
